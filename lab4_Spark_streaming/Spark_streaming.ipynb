{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c10c414",
   "metadata": {},
   "source": [
    "# Spark streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3864914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = false)\n",
      " |-- trade_id: long (nullable = false)\n",
      " |-- sequence: long (nullable = false)\n",
      " |-- time: timestamp (nullable = false)\n",
      " |-- product_id: string (nullable = false)\n",
      " |-- price: double (nullable = false)\n",
      " |-- side: string (nullable = false)\n",
      " |-- last_size: double (nullable = false)\n",
      " |-- best_bid: double (nullable = false)\n",
      " |-- best_ask: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.streaming.schemaInference\", True)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "stream = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"ws\")\n",
    "    .option(\"schema\", \"ticker\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "query = (\n",
    "    stream\n",
    "    .select(\"side\", \"product_id\", \"last_size\", \"best_bid\", \"best_ask\", \"time\")\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination(10)\n",
    "query.stop()\n",
    "stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377ecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# panic button - press only if you messed up opening new websocket and lost reference to it\n",
    "\n",
    "query.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755df1e9",
   "metadata": {},
   "source": [
    "## CoinBase stream analysis\n",
    "\n",
    "Write a query to print price (`price` parameter) in moving time window, moving along transaction time (`time` column), grouping by exchange relation (from which currency to which one, `product_id` column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "457dc66c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/28 18:09:55 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ed8821b1-0e58-48a7-ab6d-cc8964a6a0a1. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "21/11/28 18:09:55 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+----------+----------+\n",
      "|window|product_id|avg(price)|\n",
      "+------+----------+----------+\n",
      "+------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+-----------------+\n",
      "|window                                    |product_id|avg(price)       |\n",
      "+------------------------------------------+----------+-----------------+\n",
      "|{2021-11-28 18:08:00, 2021-11-28 18:10:00}|ETH-BTC   |0.07494          |\n",
      "|{2021-11-28 18:08:00, 2021-11-28 18:10:00}|ETH-USD   |4046.056666666667|\n",
      "|{2021-11-28 18:09:00, 2021-11-28 18:11:00}|ETH-BTC   |0.07494          |\n",
      "|{2021-11-28 18:09:00, 2021-11-28 18:11:00}|ETH-USD   |4046.056666666667|\n",
      "+------------------------------------------+----------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+------------------+\n",
      "|window                                    |product_id|avg(price)        |\n",
      "+------------------------------------------+----------+------------------+\n",
      "|{2021-11-28 18:08:00, 2021-11-28 18:10:00}|ETH-BTC   |0.07494           |\n",
      "|{2021-11-28 18:08:00, 2021-11-28 18:10:00}|ETH-USD   |4046.337777777778 |\n",
      "|{2021-11-28 18:09:00, 2021-11-28 18:11:00}|ETH-BTC   |0.074955          |\n",
      "|{2021-11-28 18:09:00, 2021-11-28 18:11:00}|ETH-USD   |4046.619565217392 |\n",
      "|{2021-11-28 18:10:00, 2021-11-28 18:12:00}|ETH-BTC   |0.07497           |\n",
      "|{2021-11-28 18:10:00, 2021-11-28 18:12:00}|ETH-USD   |4047.6339999999996|\n",
      "+------------------------------------------+----------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+------------------+\n",
      "|window                                    |product_id|avg(price)        |\n",
      "+------------------------------------------+----------+------------------+\n",
      "|{2021-11-28 18:08:00, 2021-11-28 18:10:00}|ETH-BTC   |0.07494           |\n",
      "|{2021-11-28 18:08:00, 2021-11-28 18:10:00}|ETH-USD   |4046.337777777778 |\n",
      "|{2021-11-28 18:09:00, 2021-11-28 18:11:00}|ETH-BTC   |0.074955          |\n",
      "|{2021-11-28 18:09:00, 2021-11-28 18:11:00}|ETH-USD   |4046.6900000000005|\n",
      "|{2021-11-28 18:10:00, 2021-11-28 18:12:00}|ETH-BTC   |0.07497           |\n",
      "|{2021-11-28 18:10:00, 2021-11-28 18:12:00}|ETH-USD   |4047.2663636363636|\n",
      "+------------------------------------------+----------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/28 18:10:05 WARN Shell: Interrupted while joining on: Thread[Thread-181350,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:10:05 WARN Shell: Interrupted while joining on: Thread[Thread-181357,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:327)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:333)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:10:05 WARN TaskSetManager: Lost task 140.0 in stage 87.0 (TID 7051) (fe2b37d904e2 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/28 18:10:05 WARN Shell: Interrupted while joining on: Thread[Thread-181355,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:10:05 WARN Shell: Interrupted while joining on: Thread[Thread-181354,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:327)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:333)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:10:05 WARN Shell: Interrupted while joining on: Thread[Thread-181352,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:10:05 WARN Shell: Interrupted while joining on: Thread[Thread-181359,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:327)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:333)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:10:05 WARN Shell: Interrupted while joining on: Thread[Thread-181356,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:360)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:327)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:333)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:10:05 WARN TaskSetManager: Lost task 141.0 in stage 87.0 (TID 7052) (fe2b37d904e2 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/28 18:10:05 WARN Shell: Interrupted while joining on: Thread[Thread-181353,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:10:05 WARN Shell: Interrupted while joining on: Thread[Thread-181351,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:327)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:333)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:10:05 WARN TaskSetManager: Lost task 128.0 in stage 87.0 (TID 7039) (fe2b37d904e2 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/28 18:10:05 WARN TaskSetManager: Lost task 131.0 in stage 87.0 (TID 7042) (fe2b37d904e2 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/28 18:10:05 WARN TaskSetManager: Lost task 133.0 in stage 87.0 (TID 7044) (fe2b37d904e2 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/28 18:10:05 WARN TaskSetManager: Lost task 132.0 in stage 87.0 (TID 7043) (fe2b37d904e2 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/28 18:10:05 WARN TaskSetManager: Lost task 134.0 in stage 87.0 (TID 7045) (fe2b37d904e2 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/28 18:10:05 WARN TaskSetManager: Lost task 136.0 in stage 87.0 (TID 7047) (fe2b37d904e2 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/28 18:10:05 WARN TaskSetManager: Lost task 135.0 in stage 87.0 (TID 7046) (fe2b37d904e2 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/28 18:10:05 WARN TaskSetManager: Lost task 138.0 in stage 87.0 (TID 7049) (fe2b37d904e2 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/28 18:10:05 WARN TaskSetManager: Lost task 137.0 in stage 87.0 (TID 7048) (fe2b37d904e2 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/28 18:10:05 WARN TaskSetManager: Lost task 139.0 in stage 87.0 (TID 7050) (fe2b37d904e2 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.streaming.schemaInference\", True)\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", True)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "stream = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"ws\")\n",
    "    .option(\"schema\", \"ticker\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "\n",
    "query = (\n",
    "    stream\n",
    "    .select(\"product_id\", \"price\", \"time\")\n",
    "    .groupBy(F.window(\"time\", \"2 minutes\", \"1 minutes\"), \"product_id\").mean(\"price\")\n",
    "    .orderBy(\"window\")\n",
    ")\n",
    "\n",
    "query = (\n",
    "    query\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination(10)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbbd102",
   "metadata": {},
   "source": [
    "## Watermarking and late data\n",
    "\n",
    "Modify previous query to show watermarks mechanism and late data handling. It should be visible in the output that relevant rows in the output table are being updated (update mode), in particular update of previous time windows after arrival of late data. Provide example output and description.\n",
    "\n",
    "Use script in `/mock` directory to generate data in batches every 10 seconds:\n",
    "- fist series: messages with timestamps 0s, 14s, 7s\n",
    "- second series: messages with timestamps 15s, 8s, 21s\n",
    "- third series: messages with timestamps 4s, 17s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d8b9772",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/28 17:45:31 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-b6a1f404-f1c6-4b73-98bd-7ce56323471d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "21/11/28 17:45:31 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+----------+----------+\n",
      "|window|product_id|avg(price)|\n",
      "+------+----------+----------+\n",
      "+------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+-----------------+\n",
      "|window                                    |product_id|avg(price)       |\n",
      "+------------------------------------------+----------+-----------------+\n",
      "|{2021-11-01 00:00:05, 2021-11-01 00:00:15}|ETH-USD   |339.4526661777917|\n",
      "|{2021-11-01 00:00:00, 2021-11-01 00:00:10}|ETH-USD   |504.2822544704219|\n",
      "|{2021-11-01 00:00:10, 2021-11-01 00:00:20}|ETH-USD   |324.4329982157316|\n",
      "|{2021-10-31 23:59:55, 2021-11-01 00:00:05}|ETH-USD   |654.092174800992 |\n",
      "+------------------------------------------+----------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------+----------+----------+\n",
      "|window|product_id|avg(price)|\n",
      "+------+----------+----------+\n",
      "+------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+------------------+\n",
      "|window                                    |product_id|avg(price)        |\n",
      "+------------------------------------------+----------+------------------+\n",
      "|{2021-11-01 00:00:05, 2021-11-01 00:00:15}|ETH-USD   |467.3129151426692 |\n",
      "|{2021-11-01 00:00:20, 2021-11-01 00:00:30}|ETH-USD   |328.9374339872395 |\n",
      "|{2021-11-01 00:00:00, 2021-11-01 00:00:10}|ETH-USD   |577.1993073377561 |\n",
      "|{2021-11-01 00:00:10, 2021-11-01 00:00:20}|ETH-USD   |163.2719779271402 |\n",
      "|{2021-11-01 00:00:15, 2021-11-01 00:00:25}|ETH-USD   |165.52419581289416|\n",
      "+------------------------------------------+----------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+------+----------+----------+\n",
      "|window|product_id|avg(price)|\n",
      "+------+----------+----------+\n",
      "+------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----------+------------------+\n",
      "|window                                    |product_id|avg(price)        |\n",
      "+------------------------------------------+----------+------------------+\n",
      "|{2021-11-01 00:00:00, 2021-11-01 00:00:10}|ETH-USD   |643.5735488840181 |\n",
      "|{2021-11-01 00:00:10, 2021-11-01 00:00:20}|ETH-USD   |190.8263802410136 |\n",
      "|{2021-10-31 23:59:55, 2021-11-01 00:00:05}|ETH-USD   |748.3942241618983 |\n",
      "|{2021-11-01 00:00:15, 2021-11-01 00:00:25}|ETH-USD   |192.32785883151624|\n",
      "+------------------------------------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.streaming.schemaInference\", True)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "stream = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"ws\")\n",
    "    .option(\"schema\", \"ticker\")\n",
    "    .option(\"url\", \"ws://mock:8025\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "\n",
    "query = (\n",
    "    stream\n",
    "    .select(\"product_id\", \"price\", \"time\")\n",
    "    .withWatermark(\"time\", \"30 seconds\")\n",
    "    .groupBy(F.window(\"time\", \"10 seconds\", \"5 seconds\"), \"product_id\").mean(\"price\")\n",
    ")\n",
    "\n",
    "query = (\n",
    "    query\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"update\")\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination(40)\n",
    "query.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35df7a5",
   "metadata": {},
   "source": [
    "### Batch 1\n",
    "\n",
    "```\n",
    "2021-10-31 23:59:55, 2021-11-01 00:00:05 -> 0s timestamp update\n",
    "2021-11-01 00:00:00, 2021-11-01 00:00:10 -> 0s, 7s timestamps updates\n",
    "2021-11-01 00:00:05, 2021-11-01 00:00:15 -> 7s, 14s timestamps updates\n",
    "2021-11-01 00:00:10, 2021-11-01 00:00:20 -> 14s timestamp update\n",
    "```\n",
    "\n",
    "### Batch 2\n",
    "\n",
    "No updates\n",
    "\n",
    "### Batch 3\n",
    "\n",
    "```\n",
    "2021-10-31 23:59:55, 2021-11-01 00:00:05 -> no updates\n",
    "2021-11-01 00:00:00, 2021-11-01 00:00:10 -> 8s timestamp update\n",
    "2021-11-01 00:00:05, 2021-11-01 00:00:15 -> 8s timestamp update\n",
    "2021-11-01 00:00:10, 2021-11-01 00:00:20 -> 15s timestamp update\n",
    "2021-11-01 00:00:15, 2021-11-01 00:00:25 -> 15s, 21s timestamps updates (new window)\n",
    "2021-11-01 00:00:20, 2021-11-01 00:00:30 -> 21s timestamp update (new window)\n",
    "```\n",
    "\n",
    "### Batch 4\n",
    "\n",
    "No updates\n",
    "\n",
    "### Batch 5\n",
    "\n",
    "```\n",
    "2021-10-31 23:59:55, 2021-11-01 00:00:05 -> 4s timestamp update\n",
    "2021-11-01 00:00:00, 2021-11-01 00:00:10 -> 4s timestamp update\n",
    "2021-11-01 00:00:05, 2021-11-01 00:00:15 -> no updates\n",
    "2021-11-01 00:00:10, 2021-11-01 00:00:20 -> 17s timestamp update\n",
    "2021-11-01 00:00:15, 2021-11-01 00:00:25 -> 17s timestamp update\n",
    "2021-11-01 00:00:20, 2021-11-01 00:00:30 -> no updates\n",
    "```\n",
    "\n",
    "30s watermark is enough to make all updates apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1246e5",
   "metadata": {},
   "source": [
    "## Stream join\n",
    "\n",
    "Divide CoinBase data from `ticker` channel into 2 streams, using subcription filtering: one stream for `size=\"sell\"`, second for `side=\"buy\"`. Then create a query to join those streams and prints transactions for given `product_id` that happened one after another during 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5ef9213",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/28 18:04:58 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-8a3e389c-a037-4e6a-ae6c-76e34d83cb0d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "21/11/28 18:04:58 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+---------------+----------+---------+--------+--------------+---------+--------+\n",
      "|sell_side|sell_product_id|sell_price|sell_time|buy_side|buy_product_id|buy_price|buy_time|\n",
      "+---------+---------------+----------+---------+--------+--------------+---------+--------+\n",
      "+---------+---------------+----------+---------+--------+--------------+---------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell_side|sell_product_id|sell_price|sell_time              |buy_side|buy_product_id|buy_price|buy_time               |\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell     |ETH-USD        |4048.37   |2021-11-28 18:04:58.761|buy     |ETH-USD       |4048.4   |2021-11-28 18:04:59.586|\n",
      "|sell     |ETH-USD        |4048.37   |2021-11-28 18:05:01.2  |buy     |ETH-USD       |4048.38  |2021-11-28 18:05:01.297|\n",
      "|sell     |ETH-USD        |4048.37   |2021-11-28 18:05:01.2  |buy     |ETH-USD       |4048.38  |2021-11-28 18:05:01.354|\n",
      "|sell     |ETH-USD        |4048.37   |2021-11-28 18:05:01.2  |buy     |ETH-USD       |4048.48  |2021-11-28 18:05:02.163|\n",
      "|sell     |ETH-USD        |4048.37   |2021-11-28 18:05:01.2  |buy     |ETH-USD       |4048.48  |2021-11-28 18:05:02.163|\n",
      "|sell     |ETH-USD        |4048.37   |2021-11-28 18:05:01.2  |buy     |ETH-USD       |4048.48  |2021-11-28 18:05:02.163|\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+---------+---------------+----------+----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell_side|sell_product_id|sell_price|sell_time             |buy_side|buy_product_id|buy_price|buy_time               |\n",
      "+---------+---------------+----------+----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell     |ETH-USD        |4048.2    |2021-11-28 18:05:03.14|buy     |ETH-USD       |4047.7   |2021-11-28 18:05:03.827|\n",
      "+---------+---------------+----------+----------------------+--------+--------------+---------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell_side|sell_product_id|sell_price|sell_time              |buy_side|buy_product_id|buy_price|buy_time               |\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell     |ETH-USD        |4046.66   |2021-11-28 18:05:05.262|buy     |ETH-USD       |4046.67  |2021-11-28 18:05:05.916|\n",
      "|sell     |ETH-USD        |4046.66   |2021-11-28 18:05:06.295|buy     |ETH-USD       |4046.81  |2021-11-28 18:05:06.834|\n",
      "|sell     |ETH-USD        |4046.53   |2021-11-28 18:05:06.295|buy     |ETH-USD       |4046.81  |2021-11-28 18:05:06.834|\n",
      "|sell     |ETH-USD        |4046.66   |2021-11-28 18:05:06.295|buy     |ETH-USD       |4046.82  |2021-11-28 18:05:06.834|\n",
      "|sell     |ETH-USD        |4046.53   |2021-11-28 18:05:06.295|buy     |ETH-USD       |4046.82  |2021-11-28 18:05:06.834|\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+---------+---------------+----------+---------------------+--------+--------------+---------+-----------------------+\n",
      "|sell_side|sell_product_id|sell_price|sell_time            |buy_side|buy_product_id|buy_price|buy_time               |\n",
      "+---------+---------------+----------+---------------------+--------+--------------+---------+-----------------------+\n",
      "|sell     |ETH-USD        |4047.79   |2021-11-28 18:05:10.2|buy     |ETH-USD       |4047.8   |2021-11-28 18:05:10.211|\n",
      "|sell     |ETH-USD        |4047.79   |2021-11-28 18:05:10.2|buy     |ETH-USD       |4047.8   |2021-11-28 18:05:10.428|\n",
      "+---------+---------------+----------+---------------------+--------+--------------+---------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell_side|sell_product_id|sell_price|sell_time              |buy_side|buy_product_id|buy_price|buy_time               |\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell     |ETH-USD        |4047.11   |2021-11-28 18:05:13.063|buy     |ETH-USD       |4046.86  |2021-11-28 18:05:13.866|\n",
      "|sell     |ETH-USD        |4046.67   |2021-11-28 18:05:13.927|buy     |ETH-USD       |4046.56  |2021-11-28 18:05:14.742|\n",
      "|sell     |ETH-USD        |4046.22   |2021-11-28 18:05:14.348|buy     |ETH-USD       |4046.56  |2021-11-28 18:05:14.742|\n",
      "|sell     |ETH-USD        |4046.24   |2021-11-28 18:05:14.389|buy     |ETH-USD       |4046.56  |2021-11-28 18:05:14.742|\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell_side|sell_product_id|sell_price|sell_time              |buy_side|buy_product_id|buy_price|buy_time               |\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell     |ETH-USD        |4046.67   |2021-11-28 18:05:15.264|buy     |ETH-USD       |4046.87  |2021-11-28 18:05:15.482|\n",
      "|sell     |ETH-USD        |4046.43   |2021-11-28 18:05:15.264|buy     |ETH-USD       |4046.87  |2021-11-28 18:05:15.482|\n",
      "|sell     |ETH-USD        |4046.67   |2021-11-28 18:05:15.264|buy     |ETH-USD       |4046.58  |2021-11-28 18:05:15.934|\n",
      "|sell     |ETH-USD        |4046.43   |2021-11-28 18:05:15.264|buy     |ETH-USD       |4046.58  |2021-11-28 18:05:15.934|\n",
      "|sell     |ETH-USD        |4046.06   |2021-11-28 18:05:16.172|buy     |ETH-USD       |4046.49  |2021-11-28 18:05:16.322|\n",
      "|sell     |ETH-USD        |4045.96   |2021-11-28 18:05:16.172|buy     |ETH-USD       |4046.49  |2021-11-28 18:05:16.322|\n",
      "|sell     |ETH-USD        |4046.06   |2021-11-28 18:05:16.172|buy     |ETH-USD       |4046.28  |2021-11-28 18:05:16.877|\n",
      "|sell     |ETH-USD        |4045.96   |2021-11-28 18:05:16.172|buy     |ETH-USD       |4046.28  |2021-11-28 18:05:16.877|\n",
      "|sell     |ETH-USD        |4045.83   |2021-11-28 18:05:16.527|buy     |ETH-USD       |4046.28  |2021-11-28 18:05:16.877|\n",
      "|sell     |ETH-USD        |4045.25   |2021-11-28 18:05:17.887|buy     |ETH-USD       |4045.2   |2021-11-28 18:05:18.058|\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell_side|sell_product_id|sell_price|sell_time              |buy_side|buy_product_id|buy_price|buy_time               |\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell     |ETH-USD        |4045.5    |2021-11-28 18:05:20.001|buy     |ETH-USD       |4045.51  |2021-11-28 18:05:20.126|\n",
      "|sell     |ETH-USD        |4045.5    |2021-11-28 18:05:20.001|buy     |ETH-USD       |4045.51  |2021-11-28 18:05:20.606|\n",
      "|sell     |ETH-USD        |4045.5    |2021-11-28 18:05:20.001|buy     |ETH-USD       |4045.52  |2021-11-28 18:05:20.606|\n",
      "|sell     |ETH-USD        |4045.3    |2021-11-28 18:05:21.054|buy     |ETH-USD       |4045.58  |2021-11-28 18:05:21.558|\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell_side|sell_product_id|sell_price|sell_time              |buy_side|buy_product_id|buy_price|buy_time               |\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell     |ETH-USD        |4045.49   |2021-11-28 18:05:23.441|buy     |ETH-USD       |4045.87  |2021-11-28 18:05:23.664|\n",
      "|sell     |ETH-USD        |4045.49   |2021-11-28 18:05:23.441|buy     |ETH-USD       |4046.09  |2021-11-28 18:05:24.004|\n",
      "|sell     |ETH-USD        |4045.25   |2021-11-28 18:05:24.353|buy     |ETH-USD       |4045.45  |2021-11-28 18:05:24.791|\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell_side|sell_product_id|sell_price|sell_time              |buy_side|buy_product_id|buy_price|buy_time               |\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell     |ETH-USD        |4045.25   |2021-11-28 18:05:24.353|buy     |ETH-USD       |4045.42  |2021-11-28 18:05:25.242|\n",
      "|sell     |ETH-USD        |4045.01   |2021-11-28 18:05:27.368|buy     |ETH-USD       |4045.42  |2021-11-28 18:05:27.573|\n",
      "|sell     |ETH-USD        |4045.25   |2021-11-28 18:05:24.353|buy     |ETH-USD       |4045.42  |2021-11-28 18:05:25.261|\n",
      "|sell     |ETH-USD        |4045.01   |2021-11-28 18:05:27.368|buy     |ETH-USD       |4045.42  |2021-11-28 18:05:27.817|\n",
      "|sell     |ETH-USD        |4045.25   |2021-11-28 18:05:24.353|buy     |ETH-USD       |4045.43  |2021-11-28 18:05:25.261|\n",
      "|sell     |ETH-USD        |4045.01   |2021-11-28 18:05:27.368|buy     |ETH-USD       |4045.42  |2021-11-28 18:05:27.817|\n",
      "|sell     |ETH-USD        |4045.24   |2021-11-28 18:05:26.373|buy     |ETH-USD       |4045.53  |2021-11-28 18:05:26.523|\n",
      "|sell     |ETH-USD        |4045.24   |2021-11-28 18:05:26.373|buy     |ETH-USD       |4045.53  |2021-11-28 18:05:26.523|\n",
      "|sell     |ETH-USD        |4045.24   |2021-11-28 18:05:26.373|buy     |ETH-USD       |4045.54  |2021-11-28 18:05:26.523|\n",
      "|sell     |ETH-USD        |4045.24   |2021-11-28 18:05:26.373|buy     |ETH-USD       |4045.58  |2021-11-28 18:05:26.523|\n",
      "|sell     |ETH-USD        |4045.24   |2021-11-28 18:05:26.373|buy     |ETH-USD       |4045.35  |2021-11-28 18:05:26.602|\n",
      "|sell     |ETH-USD        |4045.24   |2021-11-28 18:05:26.373|buy     |ETH-USD       |4045.34  |2021-11-28 18:05:27.201|\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell_side|sell_product_id|sell_price|sell_time              |buy_side|buy_product_id|buy_price|buy_time               |\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell     |ETH-USD        |4042.0    |2021-11-28 18:05:30.449|buy     |ETH-USD       |4041.65  |2021-11-28 18:05:30.66 |\n",
      "|sell     |ETH-USD        |4042.0    |2021-11-28 18:05:30.449|buy     |ETH-USD       |4041.65  |2021-11-28 18:05:30.66 |\n",
      "|sell     |ETH-USD        |4041.04   |2021-11-28 18:05:30.603|buy     |ETH-USD       |4041.65  |2021-11-28 18:05:30.66 |\n",
      "|sell     |ETH-USD        |4042.0    |2021-11-28 18:05:30.449|buy     |ETH-USD       |4041.65  |2021-11-28 18:05:31.413|\n",
      "|sell     |ETH-USD        |4042.0    |2021-11-28 18:05:30.449|buy     |ETH-USD       |4041.65  |2021-11-28 18:05:31.413|\n",
      "|sell     |ETH-USD        |4041.04   |2021-11-28 18:05:30.603|buy     |ETH-USD       |4041.65  |2021-11-28 18:05:31.413|\n",
      "|sell     |ETH-USD        |4044.8    |2021-11-28 18:05:28.206|buy     |ETH-USD       |4042.51  |2021-11-28 18:05:29.095|\n",
      "|sell     |ETH-USD        |4044.24   |2021-11-28 18:05:28.206|buy     |ETH-USD       |4042.51  |2021-11-28 18:05:29.095|\n",
      "|sell     |ETH-USD        |4044.24   |2021-11-28 18:05:28.246|buy     |ETH-USD       |4042.51  |2021-11-28 18:05:29.095|\n",
      "|sell     |ETH-USD        |4045.01   |2021-11-28 18:05:28.161|buy     |ETH-USD       |4042.51  |2021-11-28 18:05:29.095|\n",
      "|sell     |ETH-USD        |4044.14   |2021-11-28 18:05:28.246|buy     |ETH-USD       |4042.51  |2021-11-28 18:05:29.095|\n",
      "|sell     |ETH-USD        |4045.0    |2021-11-28 18:05:28.161|buy     |ETH-USD       |4042.51  |2021-11-28 18:05:29.095|\n",
      "|sell     |ETH-USD        |4044.03   |2021-11-28 18:05:28.246|buy     |ETH-USD       |4042.51  |2021-11-28 18:05:29.095|\n",
      "|sell     |ETH-USD        |4045.0    |2021-11-28 18:05:28.161|buy     |ETH-USD       |4042.51  |2021-11-28 18:05:29.095|\n",
      "|sell     |ETH-USD        |4043.14   |2021-11-28 18:05:28.345|buy     |ETH-USD       |4042.51  |2021-11-28 18:05:29.095|\n",
      "|sell     |ETH-USD        |4044.87   |2021-11-28 18:05:28.161|buy     |ETH-USD       |4042.51  |2021-11-28 18:05:29.095|\n",
      "|sell     |ETH-USD        |4043.01   |2021-11-28 18:05:28.413|buy     |ETH-USD       |4042.51  |2021-11-28 18:05:29.095|\n",
      "|sell     |ETH-USD        |4044.84   |2021-11-28 18:05:28.161|buy     |ETH-USD       |4042.51  |2021-11-28 18:05:29.095|\n",
      "|sell     |ETH-USD        |4043.01   |2021-11-28 18:05:28.451|buy     |ETH-USD       |4042.51  |2021-11-28 18:05:29.095|\n",
      "|sell     |ETH-USD        |4044.8    |2021-11-28 18:05:28.177|buy     |ETH-USD       |4042.51  |2021-11-28 18:05:29.095|\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell_side|sell_product_id|sell_price|sell_time              |buy_side|buy_product_id|buy_price|buy_time               |\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:34.663|buy     |ETH-USD       |4041.12  |2021-11-28 18:05:34.9  |\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:34.663|buy     |ETH-USD       |4041.12  |2021-11-28 18:05:35.104|\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:35.075|buy     |ETH-USD       |4041.12  |2021-11-28 18:05:35.104|\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:34.663|buy     |ETH-USD       |4041.13  |2021-11-28 18:05:35.104|\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:35.075|buy     |ETH-USD       |4041.13  |2021-11-28 18:05:35.104|\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:34.663|buy     |ETH-USD       |4041.13  |2021-11-28 18:05:35.104|\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:35.075|buy     |ETH-USD       |4041.13  |2021-11-28 18:05:35.104|\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:34.663|buy     |ETH-USD       |4041.48  |2021-11-28 18:05:35.104|\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:35.075|buy     |ETH-USD       |4041.48  |2021-11-28 18:05:35.104|\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell_side|sell_product_id|sell_price|sell_time              |buy_side|buy_product_id|buy_price|buy_time               |\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:34.663|buy     |ETH-USD       |4041.48  |2021-11-28 18:05:35.286|\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:35.075|buy     |ETH-USD       |4041.48  |2021-11-28 18:05:35.286|\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:34.663|buy     |ETH-USD       |4041.48  |2021-11-28 18:05:35.308|\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:35.075|buy     |ETH-USD       |4041.48  |2021-11-28 18:05:35.308|\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:34.663|buy     |ETH-USD       |4041.48  |2021-11-28 18:05:35.308|\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:35.075|buy     |ETH-USD       |4041.48  |2021-11-28 18:05:35.308|\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:34.663|buy     |ETH-USD       |4041.57  |2021-11-28 18:05:35.315|\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:35.075|buy     |ETH-USD       |4041.57  |2021-11-28 18:05:35.315|\n",
      "|sell     |ETH-USD        |4042.16   |2021-11-28 18:05:36.335|buy     |ETH-USD       |4042.17  |2021-11-28 18:05:36.485|\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:34.663|buy     |ETH-USD       |4041.57  |2021-11-28 18:05:35.487|\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:35.075|buy     |ETH-USD       |4041.57  |2021-11-28 18:05:35.487|\n",
      "|sell     |ETH-USD        |4042.16   |2021-11-28 18:05:36.335|buy     |ETH-USD       |4042.18  |2021-11-28 18:05:36.492|\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:34.663|buy     |ETH-USD       |4041.57  |2021-11-28 18:05:35.517|\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:35.075|buy     |ETH-USD       |4041.57  |2021-11-28 18:05:35.517|\n",
      "|sell     |ETH-USD        |4042.16   |2021-11-28 18:05:36.335|buy     |ETH-USD       |4043.27  |2021-11-28 18:05:37.292|\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:34.663|buy     |ETH-USD       |4041.57  |2021-11-28 18:05:35.519|\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:35.075|buy     |ETH-USD       |4041.57  |2021-11-28 18:05:35.519|\n",
      "|sell     |ETH-USD        |4042.16   |2021-11-28 18:05:36.335|buy     |ETH-USD       |4043.27  |2021-11-28 18:05:37.292|\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:34.663|buy     |ETH-USD       |4041.57  |2021-11-28 18:05:35.531|\n",
      "|sell     |ETH-USD        |4041.11   |2021-11-28 18:05:35.075|buy     |ETH-USD       |4041.57  |2021-11-28 18:05:35.531|\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell_side|sell_product_id|sell_price|sell_time              |buy_side|buy_product_id|buy_price|buy_time               |\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell     |ETH-USD        |4044.26   |2021-11-28 18:05:38.565|buy     |ETH-USD       |4043.84  |2021-11-28 18:05:38.834|\n",
      "|sell     |ETH-USD        |4044.26   |2021-11-28 18:05:38.578|buy     |ETH-USD       |4043.84  |2021-11-28 18:05:38.834|\n",
      "|sell     |ETH-USD        |4044.18   |2021-11-28 18:05:38.578|buy     |ETH-USD       |4043.84  |2021-11-28 18:05:38.834|\n",
      "|sell     |ETH-USD        |4043.46   |2021-11-28 18:05:38.674|buy     |ETH-USD       |4043.84  |2021-11-28 18:05:38.834|\n",
      "|sell     |ETH-USD        |4043.31   |2021-11-28 18:05:38.674|buy     |ETH-USD       |4043.84  |2021-11-28 18:05:38.834|\n",
      "|sell     |ETH-USD        |4043.3    |2021-11-28 18:05:38.674|buy     |ETH-USD       |4043.84  |2021-11-28 18:05:38.834|\n",
      "|sell     |ETH-USD        |4043.3    |2021-11-28 18:05:38.674|buy     |ETH-USD       |4043.84  |2021-11-28 18:05:38.834|\n",
      "|sell     |ETH-USD        |4043.3    |2021-11-28 18:05:38.674|buy     |ETH-USD       |4043.84  |2021-11-28 18:05:38.834|\n",
      "|sell     |ETH-USD        |4043.3    |2021-11-28 18:05:38.674|buy     |ETH-USD       |4043.84  |2021-11-28 18:05:38.834|\n",
      "|sell     |ETH-USD        |4043.28   |2021-11-28 18:05:38.674|buy     |ETH-USD       |4043.84  |2021-11-28 18:05:38.834|\n",
      "|sell     |ETH-USD        |4043.27   |2021-11-28 18:05:38.674|buy     |ETH-USD       |4043.84  |2021-11-28 18:05:38.834|\n",
      "|sell     |ETH-USD        |4044.26   |2021-11-28 18:05:38.565|buy     |ETH-USD       |4044.74  |2021-11-28 18:05:39.47 |\n",
      "|sell     |ETH-USD        |4044.26   |2021-11-28 18:05:38.578|buy     |ETH-USD       |4044.74  |2021-11-28 18:05:39.47 |\n",
      "|sell     |ETH-USD        |4044.18   |2021-11-28 18:05:38.578|buy     |ETH-USD       |4044.74  |2021-11-28 18:05:39.47 |\n",
      "|sell     |ETH-USD        |4043.46   |2021-11-28 18:05:38.674|buy     |ETH-USD       |4044.74  |2021-11-28 18:05:39.47 |\n",
      "|sell     |ETH-USD        |4043.31   |2021-11-28 18:05:38.674|buy     |ETH-USD       |4044.74  |2021-11-28 18:05:39.47 |\n",
      "|sell     |ETH-USD        |4043.3    |2021-11-28 18:05:38.674|buy     |ETH-USD       |4044.74  |2021-11-28 18:05:39.47 |\n",
      "|sell     |ETH-USD        |4043.3    |2021-11-28 18:05:38.674|buy     |ETH-USD       |4044.74  |2021-11-28 18:05:39.47 |\n",
      "|sell     |ETH-USD        |4043.3    |2021-11-28 18:05:38.674|buy     |ETH-USD       |4044.74  |2021-11-28 18:05:39.47 |\n",
      "|sell     |ETH-USD        |4043.3    |2021-11-28 18:05:38.674|buy     |ETH-USD       |4044.74  |2021-11-28 18:05:39.47 |\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell_side|sell_product_id|sell_price|sell_time              |buy_side|buy_product_id|buy_price|buy_time               |\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell     |ETH-USD        |4046.46   |2021-11-28 18:05:41.1  |buy     |ETH-USD       |4046.88  |2021-11-28 18:05:42.021|\n",
      "|sell     |ETH-USD        |4047.49   |2021-11-28 18:05:44.352|buy     |ETH-USD       |4046.88  |2021-11-28 18:05:45.05 |\n",
      "|sell     |ETH-USD        |4047.49   |2021-11-28 18:05:44.42 |buy     |ETH-USD       |4046.88  |2021-11-28 18:05:45.05 |\n",
      "|sell     |ETH-USD        |4046.87   |2021-11-28 18:05:45.014|buy     |ETH-USD       |4046.88  |2021-11-28 18:05:45.05 |\n",
      "|sell     |ETH-USD        |4047.49   |2021-11-28 18:05:44.352|buy     |ETH-USD       |4048.11  |2021-11-28 18:05:44.399|\n",
      "|sell     |ETH-USD        |4047.49   |2021-11-28 18:05:44.352|buy     |ETH-USD       |4047.43  |2021-11-28 18:05:44.763|\n",
      "|sell     |ETH-USD        |4047.49   |2021-11-28 18:05:44.42 |buy     |ETH-USD       |4047.43  |2021-11-28 18:05:44.763|\n",
      "|sell     |ETH-USD        |4047.49   |2021-11-28 18:05:44.352|buy     |ETH-USD       |4047.43  |2021-11-28 18:05:44.815|\n",
      "|sell     |ETH-USD        |4047.49   |2021-11-28 18:05:44.42 |buy     |ETH-USD       |4047.43  |2021-11-28 18:05:44.815|\n",
      "|sell     |ETH-USD        |4047.49   |2021-11-28 18:05:44.352|buy     |ETH-USD       |4047.43  |2021-11-28 18:05:44.933|\n",
      "|sell     |ETH-USD        |4047.49   |2021-11-28 18:05:44.42 |buy     |ETH-USD       |4047.43  |2021-11-28 18:05:44.933|\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 15\n",
      "-------------------------------------------\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell_side|sell_product_id|sell_price|sell_time              |buy_side|buy_product_id|buy_price|buy_time               |\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell     |ETH-USD        |4046.87   |2021-11-28 18:05:45.014|buy     |ETH-USD       |4046.08  |2021-11-28 18:05:45.434|\n",
      "|sell     |ETH-USD        |4046.87   |2021-11-28 18:05:45.014|buy     |ETH-USD       |4046.59  |2021-11-28 18:05:45.756|\n",
      "|sell     |ETH-USD        |4046.87   |2021-11-28 18:05:45.014|buy     |ETH-USD       |4046.59  |2021-11-28 18:05:45.843|\n",
      "|sell     |ETH-USD        |4046.07   |2021-11-28 18:05:46.478|buy     |ETH-USD       |4046.59  |2021-11-28 18:05:46.674|\n",
      "|sell     |ETH-USD        |4046.07   |2021-11-28 18:05:46.478|buy     |ETH-USD       |4046.6   |2021-11-28 18:05:46.674|\n",
      "|sell     |ETH-USD        |4046.07   |2021-11-28 18:05:46.478|buy     |ETH-USD       |4046.76  |2021-11-28 18:05:46.674|\n",
      "|sell     |ETH-USD        |4046.07   |2021-11-28 18:05:46.478|buy     |ETH-USD       |4046.08  |2021-11-28 18:05:46.928|\n",
      "|sell     |ETH-USD        |4046.07   |2021-11-28 18:05:46.478|buy     |ETH-USD       |4046.76  |2021-11-28 18:05:46.928|\n",
      "|sell     |ETH-USD        |4046.07   |2021-11-28 18:05:46.478|buy     |ETH-USD       |4046.95  |2021-11-28 18:05:47.093|\n",
      "|sell     |ETH-USD        |4046.07   |2021-11-28 18:05:46.478|buy     |ETH-USD       |4047.15  |2021-11-28 18:05:47.252|\n",
      "+---------+---------------+----------+-----------------------+--------+--------------+---------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 16\n",
      "-------------------------------------------\n",
      "+---------+---------------+----------+----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell_side|sell_product_id|sell_price|sell_time             |buy_side|buy_product_id|buy_price|buy_time               |\n",
      "+---------+---------------+----------+----------------------+--------+--------------+---------+-----------------------+\n",
      "|sell     |ETH-USD        |4046.76   |2021-11-28 18:05:49.36|buy     |ETH-USD       |4046.55  |2021-11-28 18:05:50.173|\n",
      "+---------+---------------+----------+----------------------+--------+--------------+---------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/28 18:05:58 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@124bdb47 is aborting.\n",
      "21/11/28 18:05:58 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@124bdb47 aborted.\n",
      "21/11/28 18:05:58 WARN Shell: Interrupted while joining on: Thread[Thread-163178,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:360)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:327)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:333)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:302)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:625)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:421)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:419)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:439)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:05:58 WARN Shell: Interrupted while joining on: Thread[Thread-163186,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:360)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:327)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:333)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:303)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:625)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:421)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:419)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:439)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:05:58 WARN Shell: Interrupted while joining on: Thread[Thread-163187,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:302)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:625)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:420)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:419)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:439)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:05:58 WARN Shell: Interrupted while joining on: Thread[Thread-163188,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:302)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:625)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:420)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:419)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:439)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:05:58 WARN Shell: Interrupted while joining on: Thread[Thread-163179,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:327)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:333)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:302)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:625)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:421)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:419)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:439)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:05:58 WARN Shell: Interrupted while joining on: Thread[Thread-163184,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:302)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:625)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:421)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:419)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:439)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:05:58 WARN Shell: Interrupted while joining on: Thread[Thread-163183,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:303)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:625)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:421)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:419)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:439)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:05:58 WARN Shell: Interrupted while joining on: Thread[Thread-163189,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:303)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:625)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:420)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:419)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:439)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:05:58 WARN Shell: Interrupted while joining on: Thread[Thread-163180,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:360)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:327)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:333)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:303)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:625)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:420)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:419)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:439)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:05:58 WARN Shell: Interrupted while joining on: Thread[Thread-163185,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.commit(SymmetricHashJoinStateManager.scala:349)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.commit(SymmetricHashJoinStateManager.scala:303)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.commitStateAndGetMetrics(StreamingSymmetricHashJoinExec.scala:625)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$23(StreamingSymmetricHashJoinExec.scala:420)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.timeTakenMs(StreamingSymmetricHashJoinExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.onOutputCompletion$1(StreamingSymmetricHashJoinExec.scala:419)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$processPartitions$25(StreamingSymmetricHashJoinExec.scala:439)\n",
      "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:05:58 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 147 (task 5227, attempt 0, stage 65.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:05:58 ERROR DataWritingSparkTask: Aborting commit for partition 147 (task 5227, attempt 0, stage 65.0)\n",
      "21/11/28 18:05:58 ERROR DataWritingSparkTask: Aborted commit for partition 147 (task 5227, attempt 0, stage 65.0)\n",
      "21/11/28 18:05:58 WARN TaskSetManager: Lost task 147.0 in stage 65.0 (TID 5227) (fe2b37d904e2 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/28 18:05:58 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 148 (task 5228, attempt 0, stage 65.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:05:58 ERROR DataWritingSparkTask: Aborting commit for partition 148 (task 5228, attempt 0, stage 65.0)\n",
      "21/11/28 18:05:58 ERROR DataWritingSparkTask: Aborted commit for partition 148 (task 5228, attempt 0, stage 65.0)\n",
      "21/11/28 18:05:58 WARN TaskSetManager: Lost task 148.0 in stage 65.0 (TID 5228) (fe2b37d904e2 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/28 18:05:58 ERROR Utils: Aborting task====>              (147 + 10) / 200]\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 149 (task 5229, attempt 0, stage 65.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:05:58 ERROR DataWritingSparkTask: Aborting commit for partition 149 (task 5229, attempt 0, stage 65.0)\n",
      "21/11/28 18:05:58 ERROR DataWritingSparkTask: Aborted commit for partition 149 (task 5229, attempt 0, stage 65.0)\n",
      "21/11/28 18:05:58 WARN TaskSetManager: Lost task 149.0 in stage 65.0 (TID 5229) (fe2b37d904e2 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/28 18:05:58 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 153 (task 5233, attempt 0, stage 65.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:05:58 ERROR DataWritingSparkTask: Aborting commit for partition 153 (task 5233, attempt 0, stage 65.0)\n",
      "21/11/28 18:05:58 ERROR DataWritingSparkTask: Aborted commit for partition 153 (task 5233, attempt 0, stage 65.0)\n",
      "21/11/28 18:05:58 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 150 (task 5230, attempt 0, stage 65.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:05:58 ERROR DataWritingSparkTask: Aborting commit for partition 150 (task 5230, attempt 0, stage 65.0)\n",
      "21/11/28 18:05:58 ERROR DataWritingSparkTask: Aborted commit for partition 150 (task 5230, attempt 0, stage 65.0)\n",
      "21/11/28 18:05:58 WARN TaskSetManager: Lost task 153.0 in stage 65.0 (TID 5233) (fe2b37d904e2 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/28 18:05:58 WARN TaskSetManager: Lost task 150.0 in stage 65.0 (TID 5230) (fe2b37d904e2 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/28 18:05:58 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 151 (task 5231, attempt 0, stage 65.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:05:58 ERROR DataWritingSparkTask: Aborting commit for partition 151 (task 5231, attempt 0, stage 65.0)\n",
      "21/11/28 18:05:58 ERROR DataWritingSparkTask: Aborted commit for partition 151 (task 5231, attempt 0, stage 65.0)\n",
      "21/11/28 18:05:58 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 152 (task 5232, attempt 0, stage 65.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:05:58 ERROR DataWritingSparkTask: Aborting commit for partition 152 (task 5232, attempt 0, stage 65.0)\n",
      "21/11/28 18:05:58 WARN TaskSetManager: Lost task 151.0 in stage 65.0 (TID 5231) (fe2b37d904e2 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/28 18:05:58 ERROR DataWritingSparkTask: Aborted commit for partition 152 (task 5232, attempt 0, stage 65.0)\n",
      "21/11/28 18:05:58 WARN TaskSetManager: Lost task 152.0 in stage 65.0 (TID 5232) (fe2b37d904e2 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/28 18:05:58 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 154 (task 5234, attempt 0, stage 65.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:05:58 ERROR DataWritingSparkTask: Aborting commit for partition 154 (task 5234, attempt 0, stage 65.0)\n",
      "21/11/28 18:05:58 ERROR DataWritingSparkTask: Aborted commit for partition 154 (task 5234, attempt 0, stage 65.0)\n",
      "21/11/28 18:05:58 WARN TaskSetManager: Lost task 154.0 in stage 65.0 (TID 5234) (fe2b37d904e2 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/28 18:05:58 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 158 (task 5238, attempt 0, stage 65.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:05:58 ERROR DataWritingSparkTask: Aborting commit for partition 158 (task 5238, attempt 0, stage 65.0)\n",
      "21/11/28 18:05:58 ERROR DataWritingSparkTask: Aborted commit for partition 158 (task 5238, attempt 0, stage 65.0)\n",
      "21/11/28 18:05:58 WARN TaskSetManager: Lost task 158.0 in stage 65.0 (TID 5238) (fe2b37d904e2 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/28 18:05:58 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 156 (task 5236, attempt 0, stage 65.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:05:58 ERROR DataWritingSparkTask: Aborting commit for partition 156 (task 5236, attempt 0, stage 65.0)\n",
      "21/11/28 18:05:58 ERROR DataWritingSparkTask: Aborted commit for partition 156 (task 5236, attempt 0, stage 65.0)\n",
      "21/11/28 18:05:58 WARN TaskSetManager: Lost task 156.0 in stage 65.0 (TID 5236) (fe2b37d904e2 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/28 18:05:58 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 157 (task 5237, attempt 0, stage 65.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:05:58 ERROR DataWritingSparkTask: Aborting commit for partition 157 (task 5237, attempt 0, stage 65.0)\n",
      "21/11/28 18:05:58 ERROR DataWritingSparkTask: Aborted commit for partition 157 (task 5237, attempt 0, stage 65.0)\n",
      "21/11/28 18:05:58 WARN TaskSetManager: Lost task 157.0 in stage 65.0 (TID 5237) (fe2b37d904e2 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/28 18:05:58 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 155 (task 5235, attempt 0, stage 65.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/11/28 18:05:58 ERROR DataWritingSparkTask: Aborting commit for partition 155 (task 5235, attempt 0, stage 65.0)\n",
      "21/11/28 18:05:58 ERROR DataWritingSparkTask: Aborted commit for partition 155 (task 5235, attempt 0, stage 65.0)\n",
      "21/11/28 18:05:58 WARN TaskSetManager: Lost task 155.0 in stage 65.0 (TID 5235) (fe2b37d904e2 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.streaming.schemaInference\", True)\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", True)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "stream = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"ws\")\n",
    "    .option(\"schema\", \"ticker\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "\n",
    "sell = (\n",
    "    stream\n",
    "    .select(\"side\", \"product_id\", \"price\", \"time\")\n",
    "    .filter(stream.side == \"sell\")\n",
    "    .withColumnRenamed(\"side\", \"sell_side\")\n",
    "    .withColumnRenamed(\"product_id\", \"sell_product_id\")\n",
    "    .withColumnRenamed(\"price\", \"sell_price\")\n",
    "    .withColumnRenamed(\"time\", \"sell_time\")\n",
    ")\n",
    "\n",
    "\n",
    "buy = (\n",
    "    stream\n",
    "    .select(\"side\", \"product_id\", \"price\", \"time\")\n",
    "    .filter(stream.side == \"buy\")\n",
    "    .withColumnRenamed(\"side\", \"buy_side\")\n",
    "    .withColumnRenamed(\"product_id\", \"buy_product_id\")\n",
    "    .withColumnRenamed(\"price\", \"buy_price\")\n",
    "    .withColumnRenamed(\"time\", \"buy_time\")\n",
    ")\n",
    "\n",
    "\n",
    "joined = (\n",
    "    sell.join(\n",
    "        buy,\n",
    "        F.expr(\n",
    "            \"\"\"\n",
    "            buy_product_id = sell_product_id AND\n",
    "            buy_time >= sell_time AND\n",
    "            buy_time <= sell_time + interval 1 seconds\n",
    "            \"\"\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "query = (\n",
    "    joined\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination(60)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cff0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
